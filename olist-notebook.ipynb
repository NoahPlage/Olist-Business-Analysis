{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# for connection with Azure SQL Database\n",
    "import pyodbc\n",
    "import urllib\n",
    "import sqlalchemy\n",
    "\n",
    "# for keeping credentials out of sight\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials and Authorization\n",
    "\n",
    "You will need to insert your database connection parameters and save as `sql-keys.env` local file.\n",
    "\n",
    "```\n",
    "DB_SERVER = \"XXXXX\"\n",
    "DB_NAME = \"XXXXX\" \n",
    "DB_USERNAME = \"XXXXX\"\n",
    "DB_PASSWORD = XXXXX\n",
    "```\n",
    "The following is the list of the connection parameters:\n",
    "- *DB_SERVER*: database server address e.g., localhost or an IP address.\n",
    "- *DB_NAME*: the name of the database that you want to connect.\n",
    "- *DB_USERNAME*: the username used to authenticate.\n",
    "- *DB_PASSWORD*: password used to authenticate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish working directory path\n",
    "# getcwd() returns current working directory\n",
    "wdir_path = os.getcwd()\n",
    "\n",
    "sql_path = os.path.join(wdir_path, \"sql-keys.env\") # absolute path of \"sql-keys.env\"\n",
    "# load the credentials into os environment \n",
    "load_dotenv(sql_path)\n",
    "# check if credentials loaded successfully\n",
    "os.environ\n",
    "\n",
    "# getting credentials information from \"sql-keys.env\"\n",
    "server = os.getenv(\"DB_SERVER\")\n",
    "database = os.getenv(\"DB_NAME\")\n",
    "username = os.getenv(\"DB_USERNAME\")\n",
    "password = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "driver = ''\n",
    "# retriving ODBC Driver information from user's computer using Pyodbc\n",
    "driver_names = [x for x in pyodbc.drivers() if x.endswith(' for SQL Server')]\n",
    "if driver_names:\n",
    "    driver = driver_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # if driver exists, create connection with Azure SQL Database\n",
    "    if driver:\n",
    "        conn_string = \"DRIVER={\" + driver + \"};SERVER=\" + server + \";DATABASE=\" + database + \";UID=\" + username + \";PWD=\" + password\n",
    "        \n",
    "        # establish the connection\n",
    "        \n",
    "        # According to SQLAlchemy's documentation, an exact PyODBC connection string can be\n",
    "        # sent in pyodbc's format directly using the parameter odbc_connect.\n",
    "        # As the delimiters need to be URL-encoded (especially the Driver), urllib.parse.quote_plus is used\n",
    "        # to encode the PyODBC connection string.\n",
    "        db_params = urllib.parse.quote_plus(conn_string)\n",
    "        engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(db_params), fast_executemany=True)\n",
    "        \n",
    "        cnxn = engine.raw_connection()\n",
    "\n",
    "        # create a cursor object\n",
    "        cursor = cnxn.cursor()\n",
    "\n",
    "        print(\"Connection to database created successfully.\")\n",
    "    else:\n",
    "        print(\"No suitable driver found. Cannot connect.\")\n",
    "except Exception as e:\n",
    "    print(\"Connection could not be made due to the following error: \\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tables into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(command, msg):\n",
    "    '''\n",
    "    Function to create table into Azure database\n",
    "    \n",
    "    Parameters:\n",
    "        command (string): SQL statement to create new table in database\n",
    "        msg (string): A string message to print that table is successfully created\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(command)\n",
    "        cnxn.commit()     \n",
    "    except Exception as e:\n",
    "        # rollback current transaction if there is an error\n",
    "        cnxn.rollback()\n",
    "        raise e\n",
    "    print(msg)\n",
    "\n",
    "# create olist_sellers table\n",
    "create_table(\"\"\"\n",
    "            IF OBJECT_ID('dbo.olist_sellers', 'U') IS NULL \n",
    "            BEGIN\n",
    "                CREATE TABLE olist_sellers\n",
    "                (\n",
    "                    seller_id VARCHAR(255) PRIMARY KEY NOT NULL, \n",
    "                    seller_zip_code_prefix INT, \n",
    "                    seller_city VARCHAR(255), \n",
    "                    seller_state VARCHAR(2)\n",
    "                )\n",
    "            END;\n",
    "            \"\"\",\n",
    "            \"olist_sellers successfully created.\")\n",
    "\n",
    "# create olist_customers table\n",
    "create_table(\"\"\"\n",
    "            IF OBJECT_ID('dbo.olist_customers', 'U') IS NULL \n",
    "            BEGIN\n",
    "                CREATE TABLE olist_customers \n",
    "                (\n",
    "                    customer_id VARCHAR(255) PRIMARY KEY NOT NULL,\n",
    "                    customer_unique_id VARCHAR(255) NOT NULL, \n",
    "                    customer_zip_code_prefix INT, \n",
    "                    customer_city VARCHAR(255), \n",
    "                    customer_state VARCHAR(2)\n",
    "                )\n",
    "            END;\n",
    "            \"\"\",\n",
    "            \"olist_customers successfully created.\")\n",
    "\n",
    "# create olist_geolocation\n",
    "create_table(\"\"\"\n",
    "            IF OBJECT_ID('dbo.olist_geolocation', 'U') IS NULL \n",
    "            BEGIN\n",
    "                CREATE TABLE olist_geolocation \n",
    "                (\n",
    "                    geolocation_zip_code_prefix INT PRIMARY KEY NOT NULL,\n",
    "                    geolocation_lat DECIMAL NOT NULL, \n",
    "                    geolocation_lng DECIMAL NOT NULL\n",
    "                )\n",
    "            END;\n",
    "            \"\"\",\n",
    "            \"olist_geolocation successfully created.\")\n",
    "\n",
    "# create olist_orders\n",
    "create_table(\"\"\"\n",
    "            IF OBJECT_ID('dbo.olist_orders', 'U') IS NULL \n",
    "            BEGIN\n",
    "                CREATE TABLE olist_orders\n",
    "                (\n",
    "                    order_id VARCHAR(34) PRIMARY KEY NOT NULL,\n",
    "                    order_item_id TINYINT,\n",
    "                    product_id VARCHAR(34),\n",
    "                    seller_id VARCHAR(34),\n",
    "                    shipping_limit_date DATETIME,\n",
    "                    price DECIMAL(9,2),\n",
    "                    freight_value DECIMAL(9,2)\n",
    "                    \n",
    "                )\n",
    "            END;\n",
    "            \"\"\",\n",
    "            \"olist_orders successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_summary(df):\n",
    "    print(\"Rows:\", df.shape[0], \"Columns:\", df.shape[1])\n",
    "\n",
    "    # determine the number of unique values in each column\n",
    "    print(\"\\nNumber of unique values in each column:\")\n",
    "    print(df.nunique(axis=0))\n",
    "\n",
    "    # determine the number of null values\n",
    "    print(\"\\nNumber of NULL values in each column:\")\n",
    "    print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers data\n",
    "\n",
    "The `olist_customers_dataset` contains location informations of the customer.\n",
    "- `customer_unique_id` is a unique id identifying customers in the system. It is an id generated at the time of **signup**.\n",
    "- `customer_id` is a temporary id generated everytime the customer places an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "cust_df = pd.read_csv(\"data/olist_customers_dataset.csv\")\n",
    "\n",
    "# view dataset\n",
    "cust_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_summary(cust_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sellers data\n",
    "\n",
    "The `olist_sellers_dataset` contains locations information of the sellers in terms of zip code, city and state. The sellers list their products on the website and are then responsible for dispatching the delivery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "seller_df = pd.read_csv(\"data/olist_sellers_dataset.csv\")\n",
    "\n",
    "# view dataset\n",
    "seller_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_summary(seller_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocation data\n",
    "\n",
    "The `olist_geolocation_dataset` contains latitude and longitude points, city and state for the zipcodes. Since every customers and sellers in the `olist_customers_dataset` and `olist_sellers_dataset` has a zipcode associated as their location within the city and state, we will be dropping `geolocation_city` and `geolocation_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "geo_df = pd.read_csv('data/olist_geolocation_dataset.csv')\n",
    "\n",
    "# view dataset\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_summary(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns geolocation_city and geolocation_state\n",
    "geo_df = geo_df.drop(columns=['geolocation_city', 'geolocation_state'])\n",
    "\n",
    "# view dateset after dropping columns\n",
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 zip codes with two or more states. To solve this problem, we will count the states by zip code and use the majority state for this zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df[geo_df['geolocation_zip_code_prefix'] == 22261].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "order_items_df = pd.read_csv('data/olist_order_items_dataset.csv')\n",
    "\n",
    "# view dataset\n",
    "order_items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_summary(order_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "orders_df = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "\n",
    "# view dataset\n",
    "orders_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_summary(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert into Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df(df_name, table_name, msg):\n",
    "    '''\n",
    "    Function to insert values from dataframe into Azure database\n",
    "    \n",
    "    Parameters:\n",
    "        df_name: DataFrame name to get data from\n",
    "        table_name (string): Table name to update in SQL Azure Database\n",
    "        msg (string): A string message to print that table is successfully updated\n",
    "    '''\n",
    "    try:\n",
    "        df_name.to_sql(table_name, engine, index=False, if_exists=\"append\", schema=\"dbo\")\n",
    "        cnxn.commit()     \n",
    "    except Exception as e:\n",
    "        # rollback current transaction if there is an error\n",
    "        cnxn.rollback()\n",
    "        raise e\n",
    "    print(msg)\n",
    "\n",
    "# insert data into olist_customers table\n",
    "write_df(cust_df, \"olist_customers\", \"olist_customers updated successfully.\")\n",
    "write_df(seller_df, \"olist_sellers\", \"olist_sellers updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### olist_order_reviews_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_id: unique review identifier\n",
    "# order_id: unique order identifier\n",
    "# review_score: Note ranging from 1 to 5 given by the customer on a satisfaction survey\n",
    "# review_comment_title: Comment title from the review left by the customer, in Portuguese\n",
    "# review_comment_message: Comment message from the review left by the customer, in Portuguese\n",
    "# review_creation_date: Shows the date in which the satisfaction survey was sent to the customer\n",
    "# review_answer_timestamp: Shows satisfaction survey answer timestamp\n",
    "\n",
    "\n",
    "# Read in olist_order_reviews_dataset.csv as pandas dataframe and parse dates of \n",
    "# \"review_creation_date\" and \"review_answer_timestamp\" columns.\n",
    "parse_dates = [\"review_creation_date\", \"review_answer_timestamp\"]\n",
    "order_reviews_data = pd.read_csv(\"olist_order_reviews_dataset.csv\",\\\n",
    "                                 infer_datetime_format = True, parse_dates = parse_dates, encoding='latin-1')\n",
    "\n",
    "\n",
    "# Drop duplicates from review_id column.\n",
    "order_reviews_data.drop_duplicates(subset = ['review_id'], inplace = True)\n",
    "\n",
    "# Preview the first 5 lines of the loaded data \n",
    "order_reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of entries in each column\n",
    "order_reviews_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique entries\n",
    "order_reviews_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "order_reviews_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dates = [\"review_creation_date\", \"review_answer_timestamp\"]\n",
    "order_reviews_data = pd.read_csv(\"olist_order_reviews_dataset.csv\",\\\n",
    "                                 infer_datetime_format = True, parse_dates = parse_dates, encoding='latin-1')\n",
    "\n",
    "# Drop columns with null values\n",
    "order_reviews_data.drop(columns = [\"review_comment_title\", \"review_comment_message\"], inplace = True)\n",
    "\n",
    "#Print dataframe\n",
    "order_reviews_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column to datetime\n",
    "order_reviews_data['review_answer_timestamp'] = order_reviews_data['review_answer_timestamp'].dt.date\n",
    "\n",
    "# Check data info\n",
    "order_reviews_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column to datetime\n",
    "order_reviews_data['review_answer_timestamp'] = pd.to_datetime(order_reviews_data['review_answer_timestamp'])\n",
    "\n",
    "# Print dataframe\n",
    "order_reviews_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "olist_order_payments_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_id: unique order identifier\n",
    "# payment_sequential: a customer may pay an order with more than one payment method. If he does so, a sequence will be created to\n",
    "# payment_type: method of payment chosen by the customer\n",
    "# payment_installments: number of installments chosen by the customer\n",
    "# payment_value: transaction value\n",
    "\n",
    "# Boleto payments: Boleto is an official (regulated by the Central Bank of Brazil) payment method in Brazil. \n",
    "# To complete a transaction, customers receive a voucher stating the amount to pay for services or goods. \n",
    "# Customers then pay the boleto before its expiration date in one of several different methods, including at authorized agencies or banks, ATMs, or online bank portals. \n",
    "# You will receive payment confirmation after 1 business day, while funds will be available for payout 2 business days after payment confirmation.\n",
    "# https://stripe.com/docs/payments/boleto \n",
    "# https://www.rapyd.net/blog/what-is-boleto-everything-you-need-to-know/ \n",
    "\n",
    "# Read in olist_order_payments_dataset.csv and make pandas dataframe\n",
    "order_payment_data = pd.read_csv(\"olist_order_payments_dataset.csv\")\n",
    "\n",
    "# Drop duplicates from order_id column\n",
    "order_payment_data.drop_duplicates(subset = ['order_id'], inplace = True)\n",
    "\n",
    "# Set index as order_id\n",
    "order_payment_data.set_index(\"order_id\", inplace = True)\n",
    "\n",
    "# Print dataframe\n",
    "order_payment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows of each column\n",
    "order_payment_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of unique entries\n",
    "order_payment_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataframe\n",
    "order_payment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "order_payment_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "order_payment_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_dates = [\"review_creation_date\", \"review_answer_timestamp\"]\n",
    "order_reviews_data = pd.read_csv(\"olist_order_reviews_dataset.csv\",\\\n",
    "                                 infer_datetime_format = True, parse_dates = parse_dates, encoding='latin-1')\n",
    "\n",
    "df_comments = olist_order_reviews.loc[:, ['review_score', 'review_comment_message']]\n",
    "df_comments = df_comments.dropna(subset=['review_comment_message'])\n",
    "df_comments = df_comments.reset_index(drop=True)\n",
    "print(f'Dataset shape: {df_comments.shape}')\n",
    "df_comments.columns = ['score', 'comment']\n",
    "df_comments.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d888e4012f630a1791b605dfe07b25ffc379762a6d5a9edc260a72f30d593570"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
